{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_Challenge_FlappyBird.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QabXdcmgqmwD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sh3986/kaggle/blob/main/project4/RL_Challenge_FlappyBird.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning Challenge : FlappyBird"
      ],
      "metadata": {
        "id": "TUVhKt2Z4Lzh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3n1FquB4DM5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "062c9d7c-4907-4368-9a89-451dfd755fdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pygame\n",
            "  Downloading pygame-2.1.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.8 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting munch\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch) (1.15.0)\n",
            "Installing collected packages: pygame, munch\n",
            "Successfully installed munch-2.5.0 pygame-2.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pygame munch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Google Drive 연동\n",
        "구글 드라이브에 본 프로젝트 폴더를 저장한 후, 구글드라이브를 마운트\n"
      ],
      "metadata": {
        "id": "di1_QoEB_tX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/Colab Notebooks/Kaggle/project_4_RL/RL_Challenge')  # 본 프로젝트 폴더 주소를 입력"
      ],
      "metadata": {
        "id": "pu_JBQxX6l7v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b1d290a-9cb4-4941-fab9-550a2dd789c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/Colab Notebooks/Kaggle/project_4_RL/RL_Challenge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlMwm9bZioMr",
        "outputId": "826ededd-7a11-4382-e942-73514db63f9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Colab Notebooks/Kaggle/project_4_RL/RL_Challenge\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*DQN* class 정의"
      ],
      "metadata": {
        "id": "5awNZMPKWwUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, channel_in=1):\n",
        "        super(DQN, self).__init__()\n",
        "        self.number_of_actions = 2\n",
        "\n",
        "        # 1, 1, 84, 84\n",
        "        self.conv1 = nn.Conv2d(channel_in, 32, kernel_size = 8, stride = 4)\n",
        "        # 1, 32, 20, 20\n",
        "        self.conv2 = nn.Conv2d(32, 64, 4, 2)\n",
        "        # 1, 64, 9, 9\n",
        "        self.conv3 = nn.Conv2d(64, 64, 3, 1)\n",
        "        # 1, 64, 7, 7\n",
        "        self.fc4 = nn.Linear(7 * 7 * 64, 512)\n",
        "        self.fc5 = nn.Linear(512, self.number_of_actions)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.fc4(x.view(x.size(0), -1)))\n",
        "        return self.fc5(x)\n"
      ],
      "metadata": {
        "id": "ECP1VSJTWvJb"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "H9dnCn_CXUjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "from glob import glob\n",
        "from collections import deque\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "\n",
        "from game import Game\n",
        "from utils import init_weights\n",
        "from munch import Munch"
      ],
      "metadata": {
        "id": "t_YHp10bqCvf"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "AFhd2GQzxWpp"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    \"game\": \"flappy\",\n",
        "    \"gamma\": 0.99,\n",
        "    \"epsilon\": 0.02,\n",
        "    \"iteration\": 1000000,\n",
        "    \"lr\": 1e-4,\n",
        "    \"use_pretrained\": False,\n",
        "    \"tag\": \"dqn\",\n",
        "    \"writer\": \"writer\"\n",
        "}\n",
        "\n",
        "args = Munch(args)"
      ],
      "metadata": {
        "id": "OI-JJA4cq5rw"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import random\n",
        "\n",
        "# buffer_limit= 50000\n",
        "buffer_limit= 1000\n",
        "batch_size = 64\n",
        "class ReplayBuffer():\n",
        "    def __init__(self):\n",
        "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
        "\n",
        "    def put(self, transition):\n",
        "        self.buffer.append(transition)\n",
        "    \n",
        "    def sample(self, n):\n",
        "        mini_batch = random.sample(self.buffer, n)\n",
        "        s_lst, a_lst, r_lst, s_prime_lst, t_lst = [], [], [], [], []\n",
        "\n",
        "        for transition in mini_batch:\n",
        "            s, a, r, s_prime, terminal = transition\n",
        "            s_lst.append(s)\n",
        "            a_lst.append(a)\n",
        "            r_lst.append(r)\n",
        "            s_prime_lst.append(s_prime)\n",
        "            t_lst.append(terminal)\n",
        "\n",
        "        return torch.cat(tuple(s for s in s_lst)), \\\n",
        "                torch.cat(tuple(a for a in a_lst)), \\\n",
        "                torch.cat(tuple(r for r in r_lst)), \\\n",
        "                torch.cat(tuple(s_prime for s_prime in s_prime_lst)), \\\n",
        "                torch.cat(tuple(t for t in t_lst))\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)\n",
        "    \n",
        "    def isfull(self):\n",
        "        return len(self.buffer) == buffer_limit"
      ],
      "metadata": {
        "id": "zuNpgahS0HEk"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DQN()\n",
        "\n",
        "if args.use_pretrained:\n",
        "    model = torch.load(\n",
        "        sorted(glob(os.path.join('ckpt', args.tag, '*.pth')))[-1]\n",
        "    )\n",
        "else:\n",
        "    os.makedirs(os.path.join('ckpt', args.tag), exist_ok = True)\n",
        "    model.apply(init_weights)\n",
        "model = model.cuda()\n",
        "start = time.time()\n",
        "\n",
        "episode = 0\n",
        "iteration = 0\n",
        "epsilon = args.epsilon\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "# instantiate game\n",
        "game = Game(game=args.game)\n",
        "high_total_reward = 0"
      ],
      "metadata": {
        "id": "3TnqSBXeqzXN"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize replay memory\n",
        "memory = ReplayBuffer()\n",
        "\n",
        "elapsed_time = 0\n",
        "action = torch.zeros([model.number_of_actions], dtype=torch.float32)\n",
        "total_reward = game.reward\n",
        "terminal = game.game_over()\n",
        "\n",
        "image_data = game.get_torch_image().cuda()\n",
        "state = image_data.unsqueeze(0)\n",
        "\n",
        "start = time.time()"
      ],
      "metadata": {
        "id": "LD3rMU258pGs"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# self = memory"
      ],
      "metadata": {
        "id": "gJI_agGrSZCb"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mini_batch = random.sample(self.buffer, 10)\n",
        "# s_lst, a_lst, r_lst, s_prime_lst, t_lst = [], [], [], [], []\n",
        "\n",
        "# for transition in mini_batch:\n",
        "#     s, a, r, s_prime, terminal = transition\n",
        "#     s_lst.append(s)\n",
        "#     a_lst.append(a)\n",
        "#     r_lst.append(r)\n",
        "#     s_prime_lst.append(s_prime)\n",
        "#     t_lst.append(terminal)\n"
      ],
      "metadata": {
        "id": "hPMPfNCzSejo"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# r_lst"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKFhivqSSoKj",
        "outputId": "4245df4d-ba76-449b-a2d3-821c0e8d8eb5"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[0.]], device='cuda:0'),\n",
              " tensor([[0.]], device='cuda:0'),\n",
              " tensor([[0.]], device='cuda:0'),\n",
              " tensor([[0.]], device='cuda:0'),\n",
              " tensor([[0.]], device='cuda:0'),\n",
              " tensor([[0.]], device='cuda:0'),\n",
              " tensor([[0.]], device='cuda:0'),\n",
              " tensor([[0.]], device='cuda:0'),\n",
              " tensor([[0.]], device='cuda:0'),\n",
              " tensor([[0.]], device='cuda:0')]"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.cat(tuple(t for t in t_lst))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "UwDLHmktSm3X",
        "outputId": "80de4231-f424-4122-ae2b-10e2a8940630"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-141-e8a145e75437>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got int"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# return torch.cat(tuple(s for s in s_lst)), \\\n",
        "#         torch.cat(tuple(a for a in a_lst)), \\\n",
        "#         torch.cat(tuple(r for r in r_lst)), \\\n",
        "#         torch.cat(tuple(s_prime for s_prime in s_prime_lst)), \\\n",
        "#         torch.cat(tuple(t for t in t_lst))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJGfrTLnSg_T",
        "outputId": "34a72967-bf2d-4f3e-ce4b-91ffd67b7891"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# s_batch, a_batch, r_batch, s_prime_batch, t_batch = memory.sample(batch_size)\n",
        "\n",
        "# s_batch = s_batch.cuda()\n",
        "# a_batch = a_batch.cuda()\n",
        "# r_batch = r_batch.cuda()\n",
        "# s_prime_batch = s_prime_batch.cuda()\n",
        "# t_batch = t_batch.cuda()\n",
        "# # get output for the next state\n",
        "# current_output = model(s_batch)\n",
        "# prime_output = model(s_prime_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "uQZVwth_MAH-",
        "outputId": "e2fefd48-dd82-410b-a892-563053ccbba9"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-135-5c799a508466>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ms_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_prime_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ms_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0ma_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mr_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-131-07253f11bc05>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mt_lst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterminal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_prime\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms_prime\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms_prime_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got int"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while iteration < args.iteration:\n",
        "    output = model(state)[0]\n",
        "    action = torch.zeros([model.number_of_actions], dtype=torch.float32)\n",
        "\n",
        "    # epsilon greedy exploration\n",
        "    random_action = False\n",
        "    \"\"\"\n",
        "    epsilon = opt.final_epsilon + ((opt.num_iters - iter) * (opt.initial_epsilon - opt.final_epsilon) / opt.num_iters)\n",
        "    \"\"\"\n",
        "    # Pick action --> random or index of maximum q value\n",
        "    coin = random.random()\n",
        "    action_index = 0\n",
        "    if coin < epsilon:\n",
        "        action_index =  random.randint(0, 1)\n",
        "    else:\n",
        "        action_index =  output.argmax().item()\n",
        "\n",
        "    action[action_index] = 1\n",
        "\n",
        "    elapsed_time = time.time() - start\n",
        "\n",
        "    # get next state and reward\n",
        "    reward = game.act(action_index)\n",
        "    terminal = int(game.game_over())\n",
        "    \n",
        "    image_data_1 = game.get_torch_image().cuda()\n",
        "\n",
        "    state_1 = image_data_1.unsqueeze(0)\n",
        "    action = action.unsqueeze(0).cuda()\n",
        "    reward = torch.from_numpy(np.array([reward], dtype=np.float32)).unsqueeze(0).cuda()\n",
        "    \n",
        "    # save transition to replay memory\n",
        "    memory.put((state, action, reward, state_1, terminal))\n",
        "    # if replay memory is full, remove the oldest transition\n",
        "    if memory.isfull():\n",
        "        break\n",
        "        for _ in range(1000):\n",
        "            memory.popleft()\n",
        "            \n",
        "    continue\n",
        "    #### minibatch Train\n",
        "    # sample random minibatch\n",
        "    \"\"\"\n",
        "    TO DO\n",
        "    \"\"\"\n",
        "    # state_batch, action_batch, reward_batch, s_prime_batch, done_batch = memory.sample(batch_size)\n",
        "    s_batch, a_batch, r_batch, s_prime_batch, t_batch = memory.sample(batch_size)\n",
        "\n",
        "    s_batch = s_batch.cuda()\n",
        "    a_batch = a_batch.cuda()\n",
        "    r_batch = r_batch.cuda()\n",
        "    s_prime_batch = s_prime_batch.cuda()\n",
        "    t_batch = t_batch.cuda()\n",
        "    # get output for the next state\n",
        "    current_output = model(s_batch)\n",
        "    prime_output = model(s_prime_batch)\n",
        "    # output_1 = model(state_1)\n",
        "\n",
        "    y_batch = torch.cat(\n",
        "                    tuple(reward if terminal else reward + args.gamma * torch.max(prediction) \n",
        "                            for reward, terminal, prediction in zip(r_batch, terminal_batch, prime_output))\n",
        "                )\n",
        "    \n",
        "    # y = reward if terminal else reward + args.gamma * torch.max(output_1)\n",
        "\n",
        "    # calculate with target network\n",
        "    q_value = torch.sum(model(state) * action, dim=1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    y = y.detach()\n",
        "    loss = criterion(q_value, y)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    state = state_1\n",
        "    iteration += 1\n",
        "    total_reward += game.reward\n",
        "\n",
        "    args.writer.add_scalar('Train/loss', loss, iteration)\n",
        "\n",
        "    if terminal:\n",
        "        args.writer.add_scalar('Episode/elapsed_time', elapsed_time, episode)\n",
        "        args.writer.add_scalar('Episode/episode', episode, episode)\n",
        "        args.writer.add_scalar('Episode/total_reward', total_reward, episode)\n",
        "        \n",
        "        game.reset_game()\n",
        "        episode += 1\n",
        "        start = time.time()\n",
        "        print('Episode {} (Iteration {}): Agent passed {} pipes!, Time: {:.3f}'.format(episode, iteration, total_reward, elapsed_time))\n",
        "        if total_reward > high_total_reward:\n",
        "            print('Weight Saved!')\n",
        "            high_total_reward = total_reward\n",
        "            torch.save(model,\n",
        "                        os.path.join('ckpt', args.tag, 'E{:07d}_S{:03d}.pth'.format(episode, int(total_reward)))\n",
        "                        )\n",
        "        total_reward = 0\n",
        "\n",
        "print(\"Saving final model\")\n",
        "torch.save(model,\n",
        "            os.path.join('ckpt', args.tag, 'E_{:07d}_S{:03d}.pth'.format(episode, int(high_total_reward)))\n",
        "            )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tc2DIycBqwu1",
        "outputId": "edf0539a-c483-441c-ac19-b5bc34669c98"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving final model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state_batch, action_batch, reward_batch, s_prime_batch = memory.sample(batch_size)"
      ],
      "metadata": {
        "id": "rXu3DQ10Klku"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "from glob import glob\n",
        "from collections import deque\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "\n",
        "from game import Game\n",
        "from utils import init_weights\n",
        "from munch import Munch\n",
        "\n",
        "def train(args):\n",
        "    model = DQN()\n",
        "    if args.use_pretrained:\n",
        "        model = torch.load(\n",
        "            sorted(glob(os.path.join('ckpt', args.tag, '*.pth')))[-1]\n",
        "        )\n",
        "    else:\n",
        "        os.makedirs(os.path.join('ckpt', args.tag), exist_ok = True)\n",
        "        model.apply(init_weights)\n",
        "    model = model.cuda()\n",
        "    start = time.time()\n",
        "\n",
        "    episode = 0\n",
        "    iteration = 0\n",
        "    epsilon = args.epsilon\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "    # instantiate game\n",
        "    game = Game(game=args.game)\n",
        "    high_total_reward = 0\n",
        "\n",
        "    # initialize replay memory\n",
        "    \"\"\"\n",
        "    TO DO\n",
        "\n",
        "    D =\n",
        "    \"\"\"\n",
        "\n",
        "    elapsed_time = 0\n",
        "    action = torch.zeros([model.number_of_actions], dtype=torch.float32)\n",
        "    total_reward = game.reward\n",
        "    terminal = game.game_over()\n",
        "\n",
        "    image_data = game.get_torch_image().cuda()\n",
        "    state = image_data.unsqueeze(0)\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    while iteration < args.iteration:\n",
        "        output = model(state)[0]\n",
        "        action = torch.zeros([model.number_of_actions], dtype=torch.float32)\n",
        "\n",
        "        # epsilon greedy exploration\n",
        "        random_action = False\n",
        "        \"\"\"\n",
        "        TO DO\n",
        "\n",
        "        random_action =\n",
        "        \"\"\"\n",
        "\n",
        "        # Pick action --> random or index of maximum q value\n",
        "        action_index = 0\n",
        "        \"\"\"\n",
        "        TO DO\n",
        "\n",
        "        action_index =\n",
        "        \"\"\"\n",
        "\n",
        "        action[action_index] = 1\n",
        "\n",
        "        elapsed_time = time.time() - start\n",
        "\n",
        "        # get next state and reward\n",
        "        reward = game.act(action_index)\n",
        "        terminal = game.game_over()\n",
        "        image_data_1 = game.get_torch_image().cuda()\n",
        "\n",
        "        state_1 = image_data_1.unsqueeze(0)\n",
        "        action = action.unsqueeze(0).cuda()\n",
        "        reward = torch.from_numpy(np.array([reward], dtype=np.float32)).unsqueeze(0).cuda()\n",
        "\n",
        "        # save transition to replay memory\n",
        "        \"\"\"\n",
        "        TO DO\n",
        "        \"\"\"\n",
        "\n",
        "        # if replay memory is full, remove the oldest transition\n",
        "        \"\"\"\n",
        "        TO DO\n",
        "        \"\"\"\n",
        "\n",
        "        # sample random minibatch\n",
        "        \"\"\"\n",
        "        TO DO\n",
        "        \"\"\"\n",
        "\n",
        "        # get output for the next state\n",
        "        output_1 = model(state_1)\n",
        "\n",
        "        y = reward if terminal else reward + args.gamma * torch.max(output_1)\n",
        "\n",
        "        # calculate with target network\n",
        "        q_value = torch.sum(model(state) * action, dim=1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        y = y.detach()\n",
        "        loss = criterion(q_value, y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        state = state_1\n",
        "        iteration += 1\n",
        "        total_reward += game.reward\n",
        "\n",
        "        args.writer.add_scalar('Train/loss', loss, iteration)\n",
        "\n",
        "        if terminal:\n",
        "            args.writer.add_scalar('Episode/elapsed_time', elapsed_time, episode)\n",
        "            args.writer.add_scalar('Episode/episode', episode, episode)\n",
        "            args.writer.add_scalar('Episode/total_reward', total_reward, episode)\n",
        "            total_reward = 0\n",
        "            game.reset_game()\n",
        "            episode += 1\n",
        "            start = time.time()\n",
        "            print('Episode {} (Iteration {}): Agent passed {} pipes!, Time: {:.3f}'.format(episode, iteration, total_reward, elapsed_time))\n",
        "            if total_reward > high_total_reward:\n",
        "                print('Weight Saved!')\n",
        "                high_total_reward = total_reward\n",
        "                torch.save(model,\n",
        "                           os.path.join('ckpt', args.tag, 'E{:07d}_S{:03d}.pth'.format(episode, int(total_reward)))\n",
        "                           )\n",
        "    print(\"Saving final model\")\n",
        "    torch.save(model,\n",
        "               os.path.join('ckpt', args.tag, 'E_{:07d}_S{:03d}.pth'.format(episode, int(high_total_reward)))\n",
        "               )"
      ],
      "metadata": {
        "id": "5-8NHrQgX1qU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from munch import Munch\n",
        "#parser = argparse.ArgumentParser(description='Deep Q Learning')\n",
        "# Simple parser\n",
        "args = {\n",
        "    \"game\": \"flappy\",\n",
        "    \"gamma\": 0.99,\n",
        "    \"epsilon\": 0.02,\n",
        "    \"iteration\": 1000000,\n",
        "    \"lr\": 1e-4,\n",
        "    \"use_pretrained\": False,\n",
        "    \"tag\": \"dqn\",\n",
        "    \"writer\": \"writer\"\n",
        "}\n",
        "args = Munch(args)\n",
        "\n",
        "args.writer = SummaryWriter(os.path.join('ckpt', args.tag))\n",
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
        "print('GPU Enabled: {}'.format(torch.cuda.is_available()))\n",
        "\n",
        "train(args)\n"
      ],
      "metadata": {
        "id": "nZxWPpsCX7Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=ckpt/dqn"
      ],
      "metadata": {
        "id": "KVY8Ylk86fV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "QabXdcmgqmwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import argparse\n",
        "from glob import glob\n",
        "\n",
        "import torch\n",
        "\n",
        "from game import Game\n",
        "from utils import Recorder\n",
        "\n",
        "def test(args):\n",
        "    model = torch.load(\n",
        "        sorted(glob(os.path.join('ckpt', args.tag, '*.pth')))[-1],\n",
        "        map_location='cpu'\n",
        "    ).eval()\n",
        "    print('Loaded model: {}'.format(sorted(glob(os.path.join('ckpt', args.tag, '*.pth')))[-1]))\n",
        "    # initialize video writer\n",
        "    video_filename = 'output_{}.avi'.format(args.tag)\n",
        "\n",
        "    dict_screen_shape = {\n",
        "        \"flappy\":(288, 512),\n",
        "    }\n",
        "    out = Recorder(video_filename=video_filename, fps=30,\n",
        "                   width=dict_screen_shape[args.game][0],\n",
        "                   height=dict_screen_shape[args.game][1])\n",
        "    total_reward_list = []\n",
        "    time_list = []\n",
        "\n",
        "    rewards = {\n",
        "        \"positive\": 1, # when the plasyer pass the pipe\n",
        "        \"tick\": 0, # at every tick\n",
        "        \"loss\": 0, # when died\n",
        "    }\n",
        "    game = Game(seed=args.seed, game=args.game, rewards=rewards)\n",
        "    for trials in range(10):\n",
        "\n",
        "        elapsed_Time = 0\n",
        "        action = torch.zeros([model.number_of_actions], dtype=torch.float32)\n",
        "        terminal = game.game_over()\n",
        "        start = time.time()\n",
        "        total_reward = 0\n",
        "\n",
        "        image_data = game.get_torch_image()\n",
        "        state = image_data.unsqueeze(0)\n",
        "        while not terminal:\n",
        "            output = model(state)[0]\n",
        "            action = torch.zeros([model.number_of_actions], dtype=torch.float32)\n",
        "            action_index = torch.argmax(output)\n",
        "            total_reward += game.act(action_index)\n",
        "            terminal = game.game_over()\n",
        "            image_data_1 = game.get_torch_image()\n",
        "            state = image_data_1.unsqueeze(0)\n",
        "\n",
        "            out.write(game.get_image())\n",
        "\n",
        "        game.reset_game()\n",
        "        total_reward_list.append(total_reward)\n",
        "        time_list.append(time.time()-start)\n",
        "        print('Game Ended!')\n",
        "        print('Total reward: {} !'.format(total_reward))\n",
        "\n",
        "    # Add summary\n",
        "    out.write_score(sum(total_reward_list), sum(time_list))\n",
        "    out.save()\n",
        "    print('Cumulated Total Reward: {}'.format(sum(total_reward_list)))\n",
        "    print('Total Run Time: {:.3f}'.format(sum(time_list)))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description='Deep Q Learning')\n",
        "    parser.add_argument('--seed', default=42, type=int,\n",
        "                        help='Random seed')\n",
        "    parser.add_argument('--game', default='flappy', type=str,\n",
        "                        help='{flappy}')\n",
        "    parser.add_argument('--tag', default=\"dqn\", type=str,\n",
        "                        help='name to save')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    test(args)"
      ],
      "metadata": {
        "id": "FTb8BqhoqlhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eHhExncNqlc4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}