{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_Challenge_FlappyBird.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QabXdcmgqmwD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sh3986/kaggle/blob/main/project4/RL_Challenge_FlappyBird.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning Challenge : FlappyBird"
      ],
      "metadata": {
        "id": "TUVhKt2Z4Lzh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "l3n1FquB4DM5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e89689fd-4a3a-442f-f83f-3718ce96b8ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.7/dist-packages (2.1.2)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.7/dist-packages (2.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pygame munch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Google Drive 연동\n",
        "구글 드라이브에 본 프로젝트 폴더를 저장한 후, 구글드라이브를 마운트\n"
      ],
      "metadata": {
        "id": "di1_QoEB_tX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/Colab Notebooks/Kaggle/project_4_RL/RL_Challenge')  # 본 프로젝트 폴더 주소를 입력"
      ],
      "metadata": {
        "id": "pu_JBQxX6l7v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29dbb3d9-c4a6-42ed-abba-1ab195cd1d2b"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/Colab Notebooks/Kaggle/project_4_RL/RL_Challenge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlMwm9bZioMr",
        "outputId": "b0dccdc5-b383-4ac4-d411-02877d276317"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Colab Notebooks/Kaggle/project_4_RL/RL_Challenge\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*DQN* class 정의"
      ],
      "metadata": {
        "id": "5awNZMPKWwUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, channel_in=1):\n",
        "        super(DQN, self).__init__()\n",
        "        self.number_of_actions = 2\n",
        "\n",
        "        # 1, 1, 84, 84\n",
        "        self.conv1 = nn.Conv2d(channel_in, 32, kernel_size = 8, stride = 4)\n",
        "        # 1, 32, 20, 20\n",
        "        self.conv2 = nn.Conv2d(32, 64, 4, 2)\n",
        "        # 1, 64, 9, 9\n",
        "        self.conv3 = nn.Conv2d(64, 64, 3, 1)\n",
        "        # 1, 64, 7, 7\n",
        "        self.fc4 = nn.Linear(7 * 7 * 64, 512)\n",
        "        self.fc5 = nn.Linear(512, self.number_of_actions)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.fc4(x.view(x.size(0), -1)))\n",
        "        return self.fc5(x)\n"
      ],
      "metadata": {
        "id": "ECP1VSJTWvJb"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "H9dnCn_CXUjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "from glob import glob\n",
        "from collections import deque\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "\n",
        "from game import Game\n",
        "from utils import init_weights\n",
        "from munch import Munch"
      ],
      "metadata": {
        "id": "t_YHp10bqCvf"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "AFhd2GQzxWpp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    \"game\": \"flappy\",\n",
        "    \"gamma\": 0.99,\n",
        "    # \"epsilon\": 0.02,\n",
        "    \"initial_epsilon\": 0.1,\n",
        "    \"final_epsilon\":1e-4,\n",
        "    \"iteration\": 1000000,\n",
        "    \"lr\": 1e-4,\n",
        "    \"use_pretrained\": False,\n",
        "    \"tag\": \"dqn\",\n",
        "    \"writer\": \"writer\",\n",
        "    \"batch_size\" : 32,\n",
        "    \"memory_size\":5000\n",
        "}\n",
        "args = Munch(args)\n",
        "\n",
        "args.writer = SummaryWriter(os.path.join('ckpt', args.tag))\n",
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
        "print('GPU Enabled: {}'.format(torch.cuda.is_available()))"
      ],
      "metadata": {
        "id": "OI-JJA4cq5rw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1fe2ed1-b1a0-44ae-9b05-ca49f16119e3"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Enabled: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import random\n",
        "\n",
        "class ReplayBuffer():\n",
        "    def __init__(self):\n",
        "        self.buffer = collections.deque(maxlen=args.memory_size)\n",
        "\n",
        "    def put(self, transition):\n",
        "        self.buffer.append(transition)\n",
        "    \n",
        "    def sample(self):\n",
        "        mini_batch = random.sample(self.buffer,  min(len(self.buffer), args.batch_size))\n",
        "        s_lst, a_lst, r_lst, s_prime_lst, done_lst = [], [], [], [], []\n",
        "\n",
        "        for transition in mini_batch:\n",
        "            s, a, r, s_prime, done = transition\n",
        "            s_lst.append(s)\n",
        "            a_lst.append(a)\n",
        "            r_lst.append(r)\n",
        "            s_prime_lst.append(s_prime)\n",
        "            done_lst.append(done)\n",
        "\n",
        "        return torch.cat(tuple(s for s in s_lst)), \\\n",
        "                torch.cat(tuple(a for a in a_lst)), \\\n",
        "                torch.cat(tuple(r for r in r_lst)), \\\n",
        "                torch.cat(tuple(s_prime for s_prime in s_prime_lst)), \\\n",
        "                torch.cat(tuple(done for done in done_lst))\n",
        "\n",
        "    def flush(self, size=1):\n",
        "        for _ in range(size):\n",
        "            self.buffer.popleft()\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)\n",
        "    \n",
        "    def isfull(self):\n",
        "        return len(self.buffer) == args.memory_size"
      ],
      "metadata": {
        "id": "zuNpgahS0HEk"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DQN()\n",
        "# model_target = DQN()\n",
        "\n",
        "if args.use_pretrained:\n",
        "    model = torch.load(\n",
        "        sorted(glob(os.path.join('ckpt', args.tag, '*.pth')))[-1]\n",
        "    )\n",
        "else:\n",
        "    os.makedirs(os.path.join('ckpt', args.tag), exist_ok = True)\n",
        "    model.apply(init_weights)\n",
        "model = model.cuda()\n",
        "start = time.time()\n",
        "\n",
        "episode = 0\n",
        "iteration = 0\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "# instantiate game\n",
        "game = Game(game=args.game)\n",
        "high_total_reward = 0"
      ],
      "metadata": {
        "id": "3TnqSBXeqzXN"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize replay memory\n",
        "memory = ReplayBuffer()\n",
        "\n",
        "elapsed_time = 0\n",
        "action = torch.zeros([model.number_of_actions], dtype=torch.float32)\n",
        "total_reward = game.reward\n",
        "terminal = game.game_over()\n",
        "\n",
        "image_data = game.get_torch_image().cuda()\n",
        "state = image_data.unsqueeze(0)\n",
        "\n",
        "start = time.time()"
      ],
      "metadata": {
        "id": "LD3rMU258pGs"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while iteration < args.iteration:\n",
        "    output = model(state)[0]\n",
        "    action = torch.zeros([model.number_of_actions], dtype=torch.float32)\n",
        "    \n",
        "    # epsilon greedy exploration\n",
        "    # Pick action --> random or index of maximum q value\n",
        "    epsilon = args.final_epsilon + ((args.iteration - iteration) * (args.initial_epsilon - args.final_epsilon) / args.iteration)\n",
        "    coin = random.random()\n",
        "    random_action = coin <= epsilon\n",
        "    if random_action:\n",
        "        action_index =  random.randint(0, 1)\n",
        "    else:\n",
        "        action_index =  output.argmax().item()\n",
        "\n",
        "    # coin = random.random()\n",
        "    # action_index = 0\n",
        "    # if coin < epsilon:\n",
        "    #     action_index =  random.randint(0, 1)\n",
        "    # else:\n",
        "    #     action_index =  output.argmax().item()\n",
        "\n",
        "    action[action_index] = 1\n",
        "\n",
        "    elapsed_time = time.time() - start\n",
        "\n",
        "    # get next state and reward\n",
        "    reward = game.act(action_index)\n",
        "    terminal = game.game_over()\n",
        "    \n",
        "    image_data_1 = game.get_torch_image().cuda()\n",
        "    state_1 = image_data_1.unsqueeze(0)\n",
        "    action = action.unsqueeze(0).cuda()\n",
        "    reward = torch.from_numpy(np.array([reward], dtype=np.float32)).unsqueeze(0).cuda()\n",
        "    done = torch.from_numpy(np.array([1.0 if terminal else 0.0], dtype=np.float32)).unsqueeze(0).cuda()\n",
        "\n",
        "    # save transition to replay memory\n",
        "    memory.put((state, action, reward, state_1, done))\n",
        "    # if replay memory is full, remove the oldest transition\n",
        "    if memory.isfull():\n",
        "        memory.flush()\n",
        "\n",
        "    #### minibatch Train\n",
        "    # sample random minibatch\n",
        "    # if (terminal) & (memory.size() > 2000):\n",
        "    s_batch, a_batch, r_batch, s_prime_batch, done_batch = memory.sample()\n",
        "    s_batch = s_batch.cuda()\n",
        "    a_batch = a_batch.cuda()\n",
        "    r_batch = r_batch.cuda()\n",
        "    s_prime_batch = s_prime_batch.cuda()\n",
        "    done_batch = done_batch.cuda()\n",
        "\n",
        "    # get output for the next state\n",
        "    q_out = model(s_batch)\n",
        "    q_prime_out = model(s_prime_batch) # q target으로 하도록\n",
        "\n",
        "    # 종료한 state는 더 이상 미래의 보상이 없으니까 (max Q 구할 필요 없음)\n",
        "    y_hat = torch.cat(\n",
        "                tuple(reward if done else reward + args.gamma * torch.max(prediction) for reward, done, prediction in zip(r_batch, done_batch, q_prime_out))\n",
        "            )\n",
        "    y_hat = y_hat.detach()\n",
        "    q_value = torch.sum(q_out * a_batch, dim=1)\n",
        "    \n",
        "    # calculate with target network\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(q_value, y_hat)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    state = state_1\n",
        "    iteration += 1\n",
        "    total_reward += game.reward\n",
        "\n",
        "    args.writer.add_scalar('Train/loss', loss, iteration)\n",
        "\n",
        "    if terminal:\n",
        "        args.writer.add_scalar('Episode/elapsed_time', elapsed_time, episode)\n",
        "        args.writer.add_scalar('Episode/episode', episode, episode)\n",
        "        args.writer.add_scalar('Episode/total_reward', total_reward, episode)\n",
        "        \n",
        "        game.reset_game()\n",
        "        episode += 1\n",
        "        start = time.time()\n",
        "        print('Episode {} (Iteration {}): Agent passed {} pipes!, Time: {:.3f} epsilon: {:.4f}'.format(episode, iteration, total_reward, elapsed_time, epsilon))\n",
        "        if total_reward > high_total_reward:\n",
        "            print('Weight Saved!')\n",
        "            high_total_reward = total_reward\n",
        "            torch.save(model,\n",
        "                        os.path.join('ckpt', args.tag, 'E{:07d}_S{:03d}.pth'.format(episode, int(total_reward)))\n",
        "                        )\n",
        "        total_reward = 0\n",
        "        # image_data = game.get_torch_image().cuda()\n",
        "        # state = image_data.unsqueeze(0)\n",
        "\n",
        "print(\"Saving final model\")\n",
        "torch.save(model,\n",
        "            os.path.join('ckpt', args.tag, 'E_{:07d}_S{:03d}.pth'.format(episode, int(high_total_reward)))\n",
        "            )"
      ],
      "metadata": {
        "id": "tc2DIycBqwu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 기존 코드"
      ],
      "metadata": {
        "id": "4_PeL9LW9i_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "from glob import glob\n",
        "from collections import deque\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "\n",
        "from game import Game\n",
        "from utils import init_weights\n",
        "from munch import Munch\n",
        "\n",
        "def train(args):\n",
        "    model = DQN()\n",
        "    if args.use_pretrained:\n",
        "        model = torch.load(\n",
        "            sorted(glob(os.path.join('ckpt', args.tag, '*.pth')))[-1]\n",
        "        )\n",
        "    else:\n",
        "        os.makedirs(os.path.join('ckpt', args.tag), exist_ok = True)\n",
        "        model.apply(init_weights)\n",
        "    model = model.cuda()\n",
        "    start = time.time()\n",
        "\n",
        "    episode = 0\n",
        "    iteration = 0\n",
        "    epsilon = args.epsilon\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "    # instantiate game\n",
        "    game = Game(game=args.game)\n",
        "    high_total_reward = 0\n",
        "\n",
        "    # initialize replay memory\n",
        "    \"\"\"\n",
        "    TO DO\n",
        "\n",
        "    D =\n",
        "    \"\"\"\n",
        "\n",
        "    elapsed_time = 0\n",
        "    action = torch.zeros([model.number_of_actions], dtype=torch.float32)\n",
        "    total_reward = game.reward\n",
        "    terminal = game.game_over()\n",
        "\n",
        "    image_data = game.get_torch_image().cuda()\n",
        "    state = image_data.unsqueeze(0)\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    while iteration < args.iteration:\n",
        "        output = model(state)[0]\n",
        "        action = torch.zeros([model.number_of_actions], dtype=torch.float32)\n",
        "\n",
        "        # epsilon greedy exploration\n",
        "        random_action = False\n",
        "        \"\"\"\n",
        "        TO DO\n",
        "\n",
        "        random_action =\n",
        "        \"\"\"\n",
        "\n",
        "        # Pick action --> random or index of maximum q value\n",
        "        action_index = 0\n",
        "        \"\"\"\n",
        "        TO DO\n",
        "\n",
        "        action_index =\n",
        "        \"\"\"\n",
        "\n",
        "        action[action_index] = 1\n",
        "\n",
        "        elapsed_time = time.time() - start\n",
        "\n",
        "        # get next state and reward\n",
        "        reward = game.act(action_index)\n",
        "        terminal = game.game_over()\n",
        "        image_data_1 = game.get_torch_image().cuda()\n",
        "\n",
        "        state_1 = image_data_1.unsqueeze(0)\n",
        "        action = action.unsqueeze(0).cuda()\n",
        "        reward = torch.from_numpy(np.array([reward], dtype=np.float32)).unsqueeze(0).cuda()\n",
        "\n",
        "        # save transition to replay memory\n",
        "        \"\"\"\n",
        "        TO DO\n",
        "        \"\"\"\n",
        "\n",
        "        # if replay memory is full, remove the oldest transition\n",
        "        \"\"\"\n",
        "        TO DO\n",
        "        \"\"\"\n",
        "\n",
        "        # sample random minibatch\n",
        "        \"\"\"\n",
        "        TO DO\n",
        "        \"\"\"\n",
        "\n",
        "        # get output for the next state\n",
        "        output_1 = model(state_1)\n",
        "\n",
        "        y = reward if terminal else reward + args.gamma * torch.max(output_1)\n",
        "\n",
        "        # calculate with target network\n",
        "        q_value = torch.sum(model(state) * action, dim=1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        y = y.detach()\n",
        "        loss = criterion(q_value, y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        state = state_1\n",
        "        iteration += 1\n",
        "        total_reward += game.reward\n",
        "\n",
        "        args.writer.add_scalar('Train/loss', loss, iteration)\n",
        "\n",
        "        if terminal:\n",
        "            args.writer.add_scalar('Episode/elapsed_time', elapsed_time, episode)\n",
        "            args.writer.add_scalar('Episode/episode', episode, episode)\n",
        "            args.writer.add_scalar('Episode/total_reward', total_reward, episode)\n",
        "            total_reward = 0\n",
        "            game.reset_game()\n",
        "            episode += 1\n",
        "            start = time.time()\n",
        "            print('Episode {} (Iteration {}): Agent passed {} pipes!, Time: {:.3f}'.format(episode, iteration, total_reward, elapsed_time))\n",
        "            if total_reward > high_total_reward:\n",
        "                print('Weight Saved!')\n",
        "                high_total_reward = total_reward\n",
        "                torch.save(model,\n",
        "                           os.path.join('ckpt', args.tag, 'E{:07d}_S{:03d}.pth'.format(episode, int(total_reward)))\n",
        "                           )\n",
        "    print(\"Saving final model\")\n",
        "    torch.save(model,\n",
        "               os.path.join('ckpt', args.tag, 'E_{:07d}_S{:03d}.pth'.format(episode, int(high_total_reward)))\n",
        "               )"
      ],
      "metadata": {
        "id": "5-8NHrQgX1qU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from munch import Munch\n",
        "#parser = argparse.ArgumentParser(description='Deep Q Learning')\n",
        "# Simple parser\n",
        "args = {\n",
        "    \"game\": \"flappy\",\n",
        "    \"gamma\": 0.99,\n",
        "    \"epsilon\": 0.02,\n",
        "    \"iteration\": 1000000,\n",
        "    \"lr\": 1e-4,\n",
        "    \"use_pretrained\": False,\n",
        "    \"tag\": \"dqn\",\n",
        "    \"writer\": \"writer\"\n",
        "}\n",
        "args = Munch(args)\n",
        "\n",
        "args.writer = SummaryWriter(os.path.join('ckpt', args.tag))\n",
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
        "print('GPU Enabled: {}'.format(torch.cuda.is_available()))\n",
        "\n",
        "train(args)\n"
      ],
      "metadata": {
        "id": "nZxWPpsCX7Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=ckpt/dqn"
      ],
      "metadata": {
        "id": "KVY8Ylk86fV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "QabXdcmgqmwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import argparse\n",
        "from glob import glob\n",
        "\n",
        "import torch\n",
        "\n",
        "from game import Game\n",
        "from utils import Recorder\n",
        "\n",
        "def test(args):\n",
        "    model = torch.load(\n",
        "        sorted(glob(os.path.join('ckpt', args.tag, '*.pth')))[-1],\n",
        "        map_location='cpu'\n",
        "    ).eval()\n",
        "    print('Loaded model: {}'.format(sorted(glob(os.path.join('ckpt', args.tag, '*.pth')))[-1]))\n",
        "    # initialize video writer\n",
        "    video_filename = 'output_{}.avi'.format(args.tag)\n",
        "\n",
        "    dict_screen_shape = {\n",
        "        \"flappy\":(288, 512),\n",
        "    }\n",
        "    out = Recorder(video_filename=video_filename, fps=30,\n",
        "                   width=dict_screen_shape[args.game][0],\n",
        "                   height=dict_screen_shape[args.game][1])\n",
        "    total_reward_list = []\n",
        "    time_list = []\n",
        "\n",
        "    rewards = {\n",
        "        \"positive\": 1, # when the plasyer pass the pipe\n",
        "        \"tick\": 0, # at every tick\n",
        "        \"loss\": 0, # when died\n",
        "    }\n",
        "    game = Game(seed=args.seed, game=args.game, rewards=rewards)\n",
        "    for trials in range(10):\n",
        "\n",
        "        elapsed_Time = 0\n",
        "        action = torch.zeros([model.number_of_actions], dtype=torch.float32)\n",
        "        terminal = game.game_over()\n",
        "        start = time.time()\n",
        "        total_reward = 0\n",
        "\n",
        "        image_data = game.get_torch_image()\n",
        "        state = image_data.unsqueeze(0)\n",
        "        while not terminal:\n",
        "            output = model(state)[0]\n",
        "            action = torch.zeros([model.number_of_actions], dtype=torch.float32)\n",
        "            action_index = torch.argmax(output)\n",
        "            total_reward += game.act(action_index)\n",
        "            terminal = game.game_over()\n",
        "            image_data_1 = game.get_torch_image()\n",
        "            state = image_data_1.unsqueeze(0)\n",
        "\n",
        "            out.write(game.get_image())\n",
        "\n",
        "        game.reset_game()\n",
        "        total_reward_list.append(total_reward)\n",
        "        time_list.append(time.time()-start)\n",
        "        print('Game Ended!')\n",
        "        print('Total reward: {} !'.format(total_reward))\n",
        "\n",
        "    # Add summary\n",
        "    out.write_score(sum(total_reward_list), sum(time_list))\n",
        "    out.save()\n",
        "    print('Cumulated Total Reward: {}'.format(sum(total_reward_list)))\n",
        "    print('Total Run Time: {:.3f}'.format(sum(time_list)))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description='Deep Q Learning')\n",
        "    parser.add_argument('--seed', default=42, type=int,\n",
        "                        help='Random seed')\n",
        "    parser.add_argument('--game', default='flappy', type=str,\n",
        "                        help='{flappy}')\n",
        "    parser.add_argument('--tag', default=\"dqn\", type=str,\n",
        "                        help='name to save')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    test(args)"
      ],
      "metadata": {
        "id": "FTb8BqhoqlhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eHhExncNqlc4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}