{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_Challenge_FlappyBird_target_net.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QabXdcmgqmwD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sh3986/kaggle/blob/main/project4/RL_Challenge_FlappyBird_target_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning Challenge : FlappyBird"
      ],
      "metadata": {
        "id": "TUVhKt2Z4Lzh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "l3n1FquB4DM5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "319d5f39-b774-45d7-ffdb-55df9bbf04f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.7/dist-packages (2.1.2)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.7/dist-packages (2.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pygame munch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Google Drive 연동\n",
        "구글 드라이브에 본 프로젝트 폴더를 저장한 후, 구글드라이브를 마운트\n"
      ],
      "metadata": {
        "id": "di1_QoEB_tX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/Colab Notebooks/Kaggle/project_4_RL/RL_Challenge')  # 본 프로젝트 폴더 주소를 입력"
      ],
      "metadata": {
        "id": "pu_JBQxX6l7v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5010aa36-3e19-4498-c84c-dc87ef21d7ca"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/Colab Notebooks/Kaggle/project_4_RL/RL_Challenge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlMwm9bZioMr",
        "outputId": "a6ac418e-ce35-4117-8373-15299ce87896"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Colab Notebooks/Kaggle/project_4_RL/RL_Challenge\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "from glob import glob\n",
        "from collections import deque\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from game import Game\n",
        "from utils import init_weights\n",
        "from munch import Munch"
      ],
      "metadata": {
        "id": "jO3rZjNH8Ogi"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*DQN* class 정의"
      ],
      "metadata": {
        "id": "5awNZMPKWwUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, channel_in=1):\n",
        "        super(DQN, self).__init__()\n",
        "        self.number_of_actions = 2\n",
        "\n",
        "        # 1, 1, 84, 84\n",
        "        self.conv1 = nn.Conv2d(channel_in, 32, kernel_size = 8, stride = 4)\n",
        "        # 1, 64, 17, 17\n",
        "        self.conv2 = nn.Conv2d(32, 64, 4, 1)\n",
        "        # 1, 128, 14, 14\n",
        "        self.conv3 = nn.Conv2d(64, 128, 4, 1)\n",
        "        # 1, 128, 12, 12\n",
        "        self.conv4 = nn.Conv2d(128, 128, 3, 1)\n",
        "        # 1, 64, 5, 5\n",
        "        self.conv5 = nn.Conv2d(128, 64, 4, 2)\n",
        "        # 1, 64, 3, 3\n",
        "        self.conv6 = nn.Conv2d(64, 64, 3, 1)\n",
        "\n",
        "        self.fc4 = nn.Linear(3 * 3 * 64, 512)\n",
        "        self.fc5 = nn.Linear(512, self.number_of_actions)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x = F.relu(self.conv6(x))\n",
        "        x = F.relu(self.fc4(x.view(x.size(0), -1)))\n",
        "        return self.fc5(x)\n"
      ],
      "metadata": {
        "id": "ECP1VSJTWvJb"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import random\n",
        "\n",
        "class ReplayBuffer():\n",
        "    def __init__(self):\n",
        "        self.buffer = collections.deque(maxlen=args.memory_size)\n",
        "\n",
        "    def put(self, transition):\n",
        "        self.buffer.append(transition)\n",
        "    \n",
        "    def sample(self):\n",
        "        mini_batch = random.sample(self.buffer,  min(len(self.buffer), args.batch_size))\n",
        "        s_lst, a_lst, r_lst, s_prime_lst, done_lst = [], [], [], [], []\n",
        "\n",
        "        for transition in mini_batch:\n",
        "            s, a, r, s_prime, done = transition\n",
        "            s_lst.append(s)\n",
        "            a_lst.append(a)\n",
        "            r_lst.append(r)\n",
        "            s_prime_lst.append(s_prime)\n",
        "            done_lst.append(done)\n",
        "\n",
        "        return torch.cat(tuple(s for s in s_lst)), \\\n",
        "                torch.cat(tuple(a for a in a_lst)), \\\n",
        "                torch.cat(tuple(r for r in r_lst)), \\\n",
        "                torch.cat(tuple(s_prime for s_prime in s_prime_lst)), \\\n",
        "                torch.cat(tuple(done for done in done_lst))\n",
        "\n",
        "    def flush(self, size=1):\n",
        "        for _ in range(size):\n",
        "            self.buffer.popleft()\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)\n",
        "    \n",
        "    def isfull(self):\n",
        "        return len(self.buffer) == args.memory_size"
      ],
      "metadata": {
        "id": "V_6UntOc9Ba8"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 파라미터 세팅"
      ],
      "metadata": {
        "id": "H9dnCn_CXUjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    \"game\": \"flappy\",\n",
        "    \"gamma\": 0.99,\n",
        "    \"initial_epsilon\": 0.1,\n",
        "    \"final_epsilon\":1e-3,\n",
        "    \"iteration\": 2000000,\n",
        "    \"lr\": 1e-4,\n",
        "    \"use_pretrained\": False,\n",
        "    \"tag\": \"dqn_target_net_4\",\n",
        "    \"writer\": \"writer\",\n",
        "    \"batch_size\" : 32,\n",
        "    \"memory_size\":10000,\n",
        "    \"update_interval\":10\n",
        "}\n",
        "args = Munch(args)\n",
        "args.writer = SummaryWriter(os.path.join('ckpt', args.tag))\n",
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
        "print('GPU Enabled: {}'.format(torch.cuda.is_available()))"
      ],
      "metadata": {
        "id": "OI-JJA4cq5rw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "127679d3-603c-4a4b-b943-5a01425bf5da"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Enabled: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 초기화"
      ],
      "metadata": {
        "id": "cqeuwFv-8HtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = DQN()\n",
        "model_target = DQN()\n",
        "model_target.load_state_dict(model.state_dict())\n",
        "\n",
        "if args.use_pretrained:\n",
        "    model = torch.load(\n",
        "        sorted(glob(os.path.join('ckpt', args.tag, '*.pth')))[-1]\n",
        "    )\n",
        "else:\n",
        "    os.makedirs(os.path.join('ckpt', args.tag), exist_ok = True)\n",
        "    model.apply(init_weights)\n",
        "\n",
        "model = model.cuda()\n",
        "model_target = model_target.cuda()\n",
        "start = time.time()\n",
        "\n",
        "episode = 0\n",
        "iteration = 0\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "# instantiate game\n",
        "memory = ReplayBuffer()\n",
        "game = Game(game=args.game)\n",
        "high_total_reward = 0\n",
        "\n",
        "elapsed_time = 0\n",
        "action = torch.zeros([model.number_of_actions], dtype=torch.float32)\n",
        "total_reward = game.reward\n",
        "terminal = game.game_over()\n",
        "\n",
        "image_data = game.get_torch_image().cuda()\n",
        "state = image_data.unsqueeze(0)\n",
        "\n",
        "start = time.time()"
      ],
      "metadata": {
        "id": "3TnqSBXeqzXN"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 강화학습"
      ],
      "metadata": {
        "id": "zs7bpTx98m9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while iteration < args.iteration:\n",
        "    output = model(state)[0]\n",
        "    action = torch.zeros([model.number_of_actions], dtype=torch.float32)\n",
        "    \n",
        "    # epsilon greedy exploration\n",
        "    # Pick action --> random or index of maximum q value\n",
        "    epsilon = args.final_epsilon + ((args.iteration - iteration) * (args.initial_epsilon - args.final_epsilon) / args.iteration)\n",
        "    coin = random.random()\n",
        "    random_action = coin <= epsilon\n",
        "    if random_action:\n",
        "        action_index =  random.randint(0, 1)\n",
        "    else:\n",
        "        action_index =  output.argmax().item()\n",
        "\n",
        "\n",
        "    action[action_index] = 1\n",
        "\n",
        "    elapsed_time = time.time() - start\n",
        "\n",
        "    # get next state and reward\n",
        "    reward = game.act(action_index)\n",
        "    terminal = game.game_over()\n",
        "    \n",
        "    image_data_1 = game.get_torch_image().cuda()\n",
        "    state_1 = image_data_1.unsqueeze(0)\n",
        "    action = action.unsqueeze(0).cuda()\n",
        "    reward = torch.from_numpy(np.array([reward], dtype=np.float32)).unsqueeze(0).cuda()\n",
        "    done = torch.from_numpy(np.array([1.0 if terminal else 0.0], dtype=np.float32)).unsqueeze(0).cuda()\n",
        "\n",
        "    # save transition to replay memory\n",
        "    memory.put((state, action, reward, state_1, done))\n",
        "\n",
        "    # if replay memory is full, remove the oldest transition\n",
        "    if memory.isfull():\n",
        "        memory.flush()\n",
        "\n",
        "    #### minibatch Train\n",
        "    # sample random minibatch\n",
        "    if terminal:\n",
        "        s_batch, a_batch, r_batch, s_prime_batch, done_batch = memory.sample()\n",
        "        s_batch = s_batch.cuda()\n",
        "        a_batch = a_batch.cuda()\n",
        "        r_batch = r_batch.cuda()\n",
        "        s_prime_batch = s_prime_batch.cuda()\n",
        "        done_batch = done_batch.cuda()\n",
        "\n",
        "        # get output for the next state\n",
        "        q_out = model(s_batch)\n",
        "        q_prime_out = model_target(s_prime_batch) # q target으로 하도록\n",
        "\n",
        "        # 종료한 state는 더 이상 미래의 보상이 없으니까 (max Q 구할 필요 없음)\n",
        "        # y_hat_list = []\n",
        "        # for reward, done, prediction in zip(r_batch, done_batch, q_prime_out):\n",
        "        #     if done:\n",
        "        #         if reward <= 2:\n",
        "        #             reward = torch.from_numpy(np.array([-100], dtype=np.float32)).unsqueeze(0)[0].cuda()\n",
        "        #         y_hat_list.append(reward)\n",
        "        #     else:\n",
        "        #         y_hat_list.append(reward + args.gamma * torch.max(prediction))\n",
        "\n",
        "        y_hat = torch.cat(\n",
        "                    tuple(reward if done else reward + args.gamma * torch.max(prediction) for reward, done, prediction in zip(r_batch, done_batch, q_prime_out))\n",
        "                )\n",
        "        # y_hat = torch.cat(tuple(val for val in y_hat_list))\n",
        "        y_hat = y_hat.detach()\n",
        "        q_value = torch.sum(q_out * a_batch, dim=1)\n",
        "        \n",
        "        # calculate with target network\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(q_value, y_hat)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        args.writer.add_scalar('Train/loss', loss, iteration)\n",
        "\n",
        "    state = state_1\n",
        "    iteration += 1\n",
        "    total_reward += game.reward\n",
        "\n",
        "    if episode%args.update_interval == 0:\n",
        "        model_target.load_state_dict(model.state_dict())\n",
        "\n",
        "\n",
        "    if terminal:\n",
        "        args.writer.add_scalar('Episode/elapsed_time', elapsed_time, episode)\n",
        "        args.writer.add_scalar('Episode/episode', episode, episode)\n",
        "        args.writer.add_scalar('Episode/total_reward', total_reward, episode)\n",
        "        \n",
        "        game.reset_game()\n",
        "        episode += 1\n",
        "        start = time.time()\n",
        "        print('Episode {} (Iteration {}): Agent passed {} pipes!, Time: {:.3f} epsilon: {:.4f} loss: {:.4f}'.format(episode, iteration, total_reward, elapsed_time, epsilon, loss))\n",
        "        if total_reward > high_total_reward:\n",
        "            print('Weight Saved!')\n",
        "            high_total_reward = total_reward\n",
        "            torch.save(model,\n",
        "                        os.path.join('ckpt', args.tag, 'E{:07d}_S{:03d}.pth'.format(episode, int(total_reward)))\n",
        "                        )\n",
        "        total_reward = 0\n",
        "        # image_data = game.get_torch_image().cuda()\n",
        "        # state = image_data.unsqueeze(0)\n",
        "\n",
        "print(\"Saving final model\")\n",
        "torch.save(model,\n",
        "            os.path.join('ckpt', args.tag, 'E_{:07d}_S{:03d}.pth'.format(episode, int(high_total_reward)))\n",
        "            )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tc2DIycBqwu1",
        "outputId": "e7c94758-27c1-45dc-ee8f-0e4284d4269f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1 (Iteration 75): Agent passed 1.0 pipes!, Time: 3.078 epsilon: 0.1000 loss: 0.0312\n",
            "Weight Saved!\n",
            "Episode 2 (Iteration 109): Agent passed 0.0 pipes!, Time: 0.137 epsilon: 0.1000 loss: 0.0007\n",
            "Episode 3 (Iteration 146): Agent passed 0.0 pipes!, Time: 0.130 epsilon: 0.1000 loss: 0.0010\n",
            "Episode 4 (Iteration 199): Agent passed 1.0 pipes!, Time: 0.213 epsilon: 0.1000 loss: 0.0001\n",
            "Episode 5 (Iteration 260): Agent passed 1.0 pipes!, Time: 0.243 epsilon: 0.1000 loss: 0.0005\n",
            "Episode 6 (Iteration 278): Agent passed 0.0 pipes!, Time: 0.066 epsilon: 0.1000 loss: 0.0005\n",
            "Episode 7 (Iteration 315): Agent passed 0.0 pipes!, Time: 0.135 epsilon: 0.1000 loss: 0.0003\n",
            "Episode 8 (Iteration 356): Agent passed 1.0 pipes!, Time: 0.142 epsilon: 0.1000 loss: 0.0000\n",
            "Episode 9 (Iteration 393): Agent passed 0.0 pipes!, Time: 0.130 epsilon: 0.1000 loss: 0.0311\n",
            "Episode 10 (Iteration 431): Agent passed 0.0 pipes!, Time: 0.132 epsilon: 0.1000 loss: 0.0006\n",
            "Episode 11 (Iteration 553): Agent passed 3.0 pipes!, Time: 0.538 epsilon: 0.1000 loss: 0.0000\n",
            "Weight Saved!\n",
            "Episode 12 (Iteration 571): Agent passed 0.0 pipes!, Time: 0.080 epsilon: 0.1000 loss: 0.0005\n",
            "Episode 13 (Iteration 652): Agent passed 2.0 pipes!, Time: 0.283 epsilon: 0.1000 loss: 0.0001\n",
            "Episode 14 (Iteration 689): Agent passed 0.0 pipes!, Time: 0.144 epsilon: 0.1000 loss: 0.0001\n",
            "Episode 15 (Iteration 707): Agent passed 0.0 pipes!, Time: 0.068 epsilon: 0.1000 loss: 0.0000\n",
            "Episode 16 (Iteration 750): Agent passed 1.0 pipes!, Time: 0.147 epsilon: 0.1000 loss: 0.0313\n",
            "Episode 17 (Iteration 787): Agent passed 0.0 pipes!, Time: 0.130 epsilon: 0.1000 loss: 0.0006\n",
            "Episode 18 (Iteration 824): Agent passed 0.0 pipes!, Time: 0.139 epsilon: 0.1000 loss: 0.0006\n",
            "Episode 19 (Iteration 863): Agent passed 0.0 pipes!, Time: 0.161 epsilon: 0.1000 loss: 0.0313\n",
            "Episode 20 (Iteration 910): Agent passed 1.0 pipes!, Time: 0.162 epsilon: 0.1000 loss: 0.0002\n",
            "Episode 21 (Iteration 946): Agent passed 0.0 pipes!, Time: 0.170 epsilon: 0.1000 loss: 0.0550\n",
            "Episode 22 (Iteration 983): Agent passed 0.0 pipes!, Time: 0.127 epsilon: 0.1000 loss: 0.0335\n",
            "Episode 23 (Iteration 1092): Agent passed 2.0 pipes!, Time: 0.378 epsilon: 0.0999 loss: 0.0002\n",
            "Episode 24 (Iteration 1112): Agent passed 0.0 pipes!, Time: 0.073 epsilon: 0.0999 loss: 0.0006\n",
            "Episode 25 (Iteration 1149): Agent passed 0.0 pipes!, Time: 0.137 epsilon: 0.0999 loss: 0.0006\n",
            "Episode 26 (Iteration 1186): Agent passed 0.0 pipes!, Time: 0.126 epsilon: 0.0999 loss: 0.0013\n",
            "Episode 27 (Iteration 1224): Agent passed 0.0 pipes!, Time: 0.143 epsilon: 0.0999 loss: 0.0028\n",
            "Episode 28 (Iteration 1261): Agent passed 0.0 pipes!, Time: 0.124 epsilon: 0.0999 loss: 0.0003\n",
            "Episode 29 (Iteration 1311): Agent passed 1.0 pipes!, Time: 0.186 epsilon: 0.0999 loss: 0.0007\n",
            "Episode 30 (Iteration 1351): Agent passed 0.0 pipes!, Time: 0.149 epsilon: 0.0999 loss: 0.0007\n",
            "Episode 31 (Iteration 1388): Agent passed 0.0 pipes!, Time: 0.165 epsilon: 0.0999 loss: 0.0009\n",
            "Episode 32 (Iteration 1461): Agent passed 1.0 pipes!, Time: 0.284 epsilon: 0.0999 loss: 0.0005\n",
            "Episode 33 (Iteration 1496): Agent passed 0.0 pipes!, Time: 0.117 epsilon: 0.0999 loss: 0.0007\n",
            "Episode 34 (Iteration 1530): Agent passed 0.0 pipes!, Time: 0.122 epsilon: 0.0999 loss: 0.0240\n",
            "Episode 35 (Iteration 1548): Agent passed 0.0 pipes!, Time: 0.061 epsilon: 0.0999 loss: 0.0001\n",
            "Episode 36 (Iteration 1585): Agent passed 0.0 pipes!, Time: 0.131 epsilon: 0.0999 loss: 0.0007\n",
            "Episode 37 (Iteration 1622): Agent passed 0.0 pipes!, Time: 0.124 epsilon: 0.0999 loss: 0.0002\n",
            "Episode 38 (Iteration 1645): Agent passed 0.0 pipes!, Time: 0.081 epsilon: 0.0999 loss: 0.0001\n",
            "Episode 39 (Iteration 1663): Agent passed 0.0 pipes!, Time: 0.064 epsilon: 0.0999 loss: 0.0001\n",
            "Episode 40 (Iteration 1681): Agent passed 0.0 pipes!, Time: 0.069 epsilon: 0.0999 loss: 0.0000\n",
            "Episode 41 (Iteration 1718): Agent passed 0.0 pipes!, Time: 0.176 epsilon: 0.0999 loss: 0.0311\n",
            "Episode 42 (Iteration 1791): Agent passed 1.0 pipes!, Time: 0.278 epsilon: 0.0999 loss: 0.0315\n",
            "Episode 43 (Iteration 1828): Agent passed 0.0 pipes!, Time: 0.128 epsilon: 0.0999 loss: 0.0017\n",
            "Episode 44 (Iteration 1863): Agent passed 0.0 pipes!, Time: 0.126 epsilon: 0.0999 loss: 0.0001\n",
            "Episode 45 (Iteration 1900): Agent passed 0.0 pipes!, Time: 0.125 epsilon: 0.0999 loss: 0.0006\n",
            "Episode 46 (Iteration 1945): Agent passed 1.0 pipes!, Time: 0.167 epsilon: 0.0999 loss: 0.0012\n",
            "Episode 47 (Iteration 2034): Agent passed 2.0 pipes!, Time: 0.326 epsilon: 0.0999 loss: 0.0000\n",
            "Episode 48 (Iteration 2087): Agent passed 1.0 pipes!, Time: 0.187 epsilon: 0.0999 loss: 0.0308\n",
            "Episode 49 (Iteration 2124): Agent passed 0.0 pipes!, Time: 0.135 epsilon: 0.0999 loss: 0.0005\n",
            "Episode 50 (Iteration 2160): Agent passed 0.0 pipes!, Time: 0.127 epsilon: 0.0999 loss: 0.0005\n",
            "Episode 51 (Iteration 2197): Agent passed 0.0 pipes!, Time: 0.176 epsilon: 0.0999 loss: 0.0005\n",
            "Episode 52 (Iteration 2306): Agent passed 2.0 pipes!, Time: 0.394 epsilon: 0.0999 loss: 0.0010\n",
            "Episode 53 (Iteration 2380): Agent passed 1.0 pipes!, Time: 0.285 epsilon: 0.0999 loss: 0.0635\n",
            "Episode 54 (Iteration 2431): Agent passed 1.0 pipes!, Time: 0.182 epsilon: 0.0999 loss: 0.0005\n",
            "Episode 55 (Iteration 2504): Agent passed 1.0 pipes!, Time: 0.282 epsilon: 0.0999 loss: 0.0311\n",
            "Episode 56 (Iteration 2541): Agent passed 0.0 pipes!, Time: 0.131 epsilon: 0.0999 loss: 0.0009\n",
            "Episode 57 (Iteration 2578): Agent passed 0.0 pipes!, Time: 0.145 epsilon: 0.0999 loss: 0.0303\n",
            "Episode 58 (Iteration 2693): Agent passed 3.0 pipes!, Time: 0.412 epsilon: 0.0999 loss: 0.0020\n",
            "Episode 59 (Iteration 2711): Agent passed 0.0 pipes!, Time: 0.072 epsilon: 0.0999 loss: 0.0006\n",
            "Episode 60 (Iteration 2748): Agent passed 0.0 pipes!, Time: 0.150 epsilon: 0.0999 loss: 0.0004\n",
            "Episode 61 (Iteration 2766): Agent passed 0.0 pipes!, Time: 0.078 epsilon: 0.0999 loss: 0.0317\n",
            "Episode 62 (Iteration 2803): Agent passed 0.0 pipes!, Time: 0.142 epsilon: 0.0999 loss: 0.0309\n",
            "Episode 63 (Iteration 2842): Agent passed 0.0 pipes!, Time: 0.150 epsilon: 0.0999 loss: 0.0014\n",
            "Episode 64 (Iteration 2883): Agent passed 1.0 pipes!, Time: 0.155 epsilon: 0.0999 loss: 0.0308\n",
            "Episode 65 (Iteration 2926): Agent passed 1.0 pipes!, Time: 0.162 epsilon: 0.0999 loss: 0.0005\n",
            "Episode 66 (Iteration 2944): Agent passed 0.0 pipes!, Time: 0.066 epsilon: 0.0999 loss: 0.0015\n",
            "Episode 67 (Iteration 2994): Agent passed 1.0 pipes!, Time: 0.184 epsilon: 0.0999 loss: 0.0020\n",
            "Episode 68 (Iteration 3012): Agent passed 0.0 pipes!, Time: 0.068 epsilon: 0.0999 loss: 0.0002\n",
            "Episode 69 (Iteration 3056): Agent passed 1.0 pipes!, Time: 0.151 epsilon: 0.0998 loss: 0.0617\n",
            "Episode 70 (Iteration 3093): Agent passed 0.0 pipes!, Time: 0.129 epsilon: 0.0998 loss: 0.0020\n",
            "Episode 71 (Iteration 3130): Agent passed 0.0 pipes!, Time: 0.142 epsilon: 0.0998 loss: 0.0012\n",
            "Episode 72 (Iteration 3167): Agent passed 0.0 pipes!, Time: 0.136 epsilon: 0.0998 loss: 0.0319\n",
            "Episode 73 (Iteration 3204): Agent passed 0.0 pipes!, Time: 0.127 epsilon: 0.0998 loss: 0.0001\n",
            "Episode 74 (Iteration 3241): Agent passed 0.0 pipes!, Time: 0.139 epsilon: 0.0998 loss: 0.0009\n",
            "Episode 75 (Iteration 3278): Agent passed 0.0 pipes!, Time: 0.132 epsilon: 0.0998 loss: 0.0008\n",
            "Episode 76 (Iteration 3315): Agent passed 0.0 pipes!, Time: 0.139 epsilon: 0.0998 loss: 0.0512\n",
            "Episode 77 (Iteration 3365): Agent passed 1.0 pipes!, Time: 0.186 epsilon: 0.0998 loss: 0.0316\n",
            "Episode 78 (Iteration 3402): Agent passed 0.0 pipes!, Time: 0.141 epsilon: 0.0998 loss: 0.0008\n",
            "Episode 79 (Iteration 3439): Agent passed 0.0 pipes!, Time: 0.131 epsilon: 0.0998 loss: 0.0292\n",
            "Episode 80 (Iteration 3476): Agent passed 0.0 pipes!, Time: 0.148 epsilon: 0.0998 loss: 0.0049\n",
            "Episode 81 (Iteration 3513): Agent passed 0.0 pipes!, Time: 0.151 epsilon: 0.0998 loss: 0.0312\n",
            "Episode 82 (Iteration 3553): Agent passed 0.0 pipes!, Time: 0.148 epsilon: 0.0998 loss: 0.0001\n",
            "Episode 83 (Iteration 3590): Agent passed 0.0 pipes!, Time: 0.131 epsilon: 0.0998 loss: 0.0001\n",
            "Episode 84 (Iteration 3631): Agent passed 1.0 pipes!, Time: 0.333 epsilon: 0.0998 loss: 0.0302\n",
            "Episode 85 (Iteration 3673): Agent passed 1.0 pipes!, Time: 0.143 epsilon: 0.0998 loss: 0.0001\n",
            "Episode 86 (Iteration 3746): Agent passed 1.0 pipes!, Time: 0.263 epsilon: 0.0998 loss: 0.0017\n",
            "Episode 87 (Iteration 3783): Agent passed 0.0 pipes!, Time: 0.123 epsilon: 0.0998 loss: 0.0013\n",
            "Episode 88 (Iteration 3825): Agent passed 1.0 pipes!, Time: 0.144 epsilon: 0.0998 loss: 0.0014\n",
            "Episode 89 (Iteration 3878): Agent passed 1.0 pipes!, Time: 0.184 epsilon: 0.0998 loss: 0.0001\n",
            "Episode 90 (Iteration 3931): Agent passed 1.0 pipes!, Time: 0.195 epsilon: 0.0998 loss: 0.0017\n",
            "Episode 91 (Iteration 3950): Agent passed 0.0 pipes!, Time: 0.083 epsilon: 0.0998 loss: 0.0028\n",
            "Episode 92 (Iteration 3988): Agent passed 0.0 pipes!, Time: 0.134 epsilon: 0.0998 loss: 0.0001\n",
            "Episode 93 (Iteration 4027): Agent passed 0.0 pipes!, Time: 0.131 epsilon: 0.0998 loss: 0.0010\n",
            "Episode 94 (Iteration 4064): Agent passed 0.0 pipes!, Time: 0.133 epsilon: 0.0998 loss: 0.0003\n",
            "Episode 95 (Iteration 4101): Agent passed 0.0 pipes!, Time: 0.124 epsilon: 0.0998 loss: 0.0010\n",
            "Episode 96 (Iteration 4138): Agent passed 0.0 pipes!, Time: 0.125 epsilon: 0.0998 loss: 0.0010\n",
            "Episode 97 (Iteration 4175): Agent passed 0.0 pipes!, Time: 0.129 epsilon: 0.0998 loss: 0.0318\n",
            "Episode 98 (Iteration 4275): Agent passed 2.0 pipes!, Time: 0.355 epsilon: 0.0998 loss: 0.0007\n",
            "Episode 99 (Iteration 4348): Agent passed 1.0 pipes!, Time: 0.268 epsilon: 0.0998 loss: 0.0508\n",
            "Episode 100 (Iteration 4385): Agent passed 0.0 pipes!, Time: 0.130 epsilon: 0.0998 loss: 0.0306\n",
            "Episode 101 (Iteration 4458): Agent passed 1.0 pipes!, Time: 0.295 epsilon: 0.0998 loss: 0.0006\n",
            "Episode 102 (Iteration 4495): Agent passed 0.0 pipes!, Time: 0.141 epsilon: 0.0998 loss: 0.0309\n",
            "Episode 103 (Iteration 4513): Agent passed 0.0 pipes!, Time: 0.062 epsilon: 0.0998 loss: 0.0082\n",
            "Episode 104 (Iteration 4618): Agent passed 2.0 pipes!, Time: 0.375 epsilon: 0.0998 loss: 0.0011\n",
            "Episode 105 (Iteration 4636): Agent passed 0.0 pipes!, Time: 0.065 epsilon: 0.0998 loss: 0.0005\n",
            "Episode 106 (Iteration 4654): Agent passed 0.0 pipes!, Time: 0.064 epsilon: 0.0998 loss: 0.0001\n",
            "Episode 107 (Iteration 4727): Agent passed 1.0 pipes!, Time: 0.249 epsilon: 0.0998 loss: 0.0040\n",
            "Episode 108 (Iteration 4764): Agent passed 0.0 pipes!, Time: 0.139 epsilon: 0.0998 loss: 0.0351\n",
            "Episode 109 (Iteration 4811): Agent passed 1.0 pipes!, Time: 0.300 epsilon: 0.0998 loss: 0.0341\n",
            "Episode 110 (Iteration 4854): Agent passed 1.0 pipes!, Time: 0.154 epsilon: 0.0998 loss: 0.0313\n",
            "Episode 111 (Iteration 4886): Agent passed 0.0 pipes!, Time: 0.133 epsilon: 0.0998 loss: 0.0001\n",
            "Episode 112 (Iteration 4923): Agent passed 0.0 pipes!, Time: 0.122 epsilon: 0.0998 loss: 0.0333\n",
            "Episode 113 (Iteration 4941): Agent passed 0.0 pipes!, Time: 0.173 epsilon: 0.0998 loss: 0.0077\n",
            "Episode 114 (Iteration 4959): Agent passed 0.0 pipes!, Time: 0.060 epsilon: 0.0998 loss: 0.0026\n",
            "Episode 115 (Iteration 4977): Agent passed 0.0 pipes!, Time: 0.059 epsilon: 0.0998 loss: 0.0010\n",
            "Episode 116 (Iteration 5014): Agent passed 0.0 pipes!, Time: 0.128 epsilon: 0.0998 loss: 0.0002\n",
            "Episode 117 (Iteration 5062): Agent passed 1.0 pipes!, Time: 0.163 epsilon: 0.0997 loss: 0.0170\n",
            "Episode 118 (Iteration 5135): Agent passed 1.0 pipes!, Time: 0.407 epsilon: 0.0997 loss: 0.0025\n",
            "Episode 119 (Iteration 5178): Agent passed 1.0 pipes!, Time: 0.154 epsilon: 0.0997 loss: 0.0006\n",
            "Episode 120 (Iteration 5215): Agent passed 0.0 pipes!, Time: 0.140 epsilon: 0.0997 loss: 0.0326\n",
            "Episode 121 (Iteration 5265): Agent passed 1.0 pipes!, Time: 0.229 epsilon: 0.0997 loss: 0.0028\n",
            "Episode 122 (Iteration 5317): Agent passed 1.0 pipes!, Time: 0.200 epsilon: 0.0997 loss: 0.0012\n",
            "Episode 123 (Iteration 5354): Agent passed 0.0 pipes!, Time: 0.126 epsilon: 0.0997 loss: 0.0283\n",
            "Episode 124 (Iteration 5391): Agent passed 0.0 pipes!, Time: 0.127 epsilon: 0.0997 loss: 0.0181\n",
            "Episode 125 (Iteration 5428): Agent passed 0.0 pipes!, Time: 0.140 epsilon: 0.0997 loss: 0.0137\n",
            "Episode 126 (Iteration 5465): Agent passed 0.0 pipes!, Time: 0.139 epsilon: 0.0997 loss: 0.0354\n",
            "Episode 127 (Iteration 5517): Agent passed 1.0 pipes!, Time: 0.187 epsilon: 0.0997 loss: 0.0483\n",
            "Episode 128 (Iteration 5554): Agent passed 0.0 pipes!, Time: 0.126 epsilon: 0.0997 loss: 0.0039\n",
            "Episode 129 (Iteration 5586): Agent passed 0.0 pipes!, Time: 0.119 epsilon: 0.0997 loss: 0.0041\n",
            "Episode 130 (Iteration 5622): Agent passed 0.0 pipes!, Time: 0.126 epsilon: 0.0997 loss: 0.0024\n",
            "Episode 131 (Iteration 5755): Agent passed 3.0 pipes!, Time: 0.555 epsilon: 0.0997 loss: 0.0015\n",
            "Episode 132 (Iteration 5792): Agent passed 0.0 pipes!, Time: 0.129 epsilon: 0.0997 loss: 0.0363\n",
            "Episode 133 (Iteration 5828): Agent passed 0.0 pipes!, Time: 0.137 epsilon: 0.0997 loss: 0.0027\n",
            "Episode 134 (Iteration 5865): Agent passed 0.0 pipes!, Time: 0.121 epsilon: 0.0997 loss: 0.0110\n",
            "Episode 135 (Iteration 5911): Agent passed 1.0 pipes!, Time: 0.158 epsilon: 0.0997 loss: 0.0127\n",
            "Episode 136 (Iteration 5952): Agent passed 1.0 pipes!, Time: 0.154 epsilon: 0.0997 loss: 0.0670\n",
            "Episode 137 (Iteration 5989): Agent passed 0.0 pipes!, Time: 0.133 epsilon: 0.0997 loss: 0.0311\n",
            "Episode 138 (Iteration 6020): Agent passed 0.0 pipes!, Time: 0.102 epsilon: 0.0997 loss: 0.0358\n",
            "Episode 139 (Iteration 6038): Agent passed 0.0 pipes!, Time: 0.062 epsilon: 0.0997 loss: 0.0347\n",
            "Episode 140 (Iteration 6075): Agent passed 0.0 pipes!, Time: 0.127 epsilon: 0.0997 loss: 0.0067\n",
            "Episode 141 (Iteration 6117): Agent passed 1.0 pipes!, Time: 0.161 epsilon: 0.0997 loss: 0.0134\n",
            "Episode 142 (Iteration 6158): Agent passed 1.0 pipes!, Time: 0.138 epsilon: 0.0997 loss: 0.0001\n",
            "Episode 143 (Iteration 6195): Agent passed 0.0 pipes!, Time: 0.137 epsilon: 0.0997 loss: 0.0660\n",
            "Episode 144 (Iteration 6232): Agent passed 0.0 pipes!, Time: 0.141 epsilon: 0.0997 loss: 0.0002\n",
            "Episode 145 (Iteration 6273): Agent passed 1.0 pipes!, Time: 0.136 epsilon: 0.0997 loss: 0.0353\n",
            "Episode 146 (Iteration 6322): Agent passed 1.0 pipes!, Time: 0.167 epsilon: 0.0997 loss: 0.0108\n",
            "Episode 147 (Iteration 6365): Agent passed 1.0 pipes!, Time: 0.147 epsilon: 0.0997 loss: 0.0005\n",
            "Episode 148 (Iteration 6412): Agent passed 1.0 pipes!, Time: 0.170 epsilon: 0.0997 loss: 0.0593\n",
            "Episode 149 (Iteration 6461): Agent passed 1.0 pipes!, Time: 0.173 epsilon: 0.0997 loss: 0.0061\n",
            "Episode 150 (Iteration 6523): Agent passed 1.0 pipes!, Time: 0.226 epsilon: 0.0997 loss: 0.0096\n",
            "Episode 151 (Iteration 6560): Agent passed 0.0 pipes!, Time: 0.153 epsilon: 0.0997 loss: 0.0003\n",
            "Episode 152 (Iteration 6639): Agent passed 2.0 pipes!, Time: 0.273 epsilon: 0.0997 loss: 0.0297\n",
            "Episode 153 (Iteration 6748): Agent passed 2.0 pipes!, Time: 0.380 epsilon: 0.0997 loss: 0.0277\n",
            "Episode 154 (Iteration 6801): Agent passed 1.0 pipes!, Time: 0.174 epsilon: 0.0997 loss: 0.0295\n",
            "Episode 155 (Iteration 6837): Agent passed 0.0 pipes!, Time: 0.125 epsilon: 0.0997 loss: 0.0336\n",
            "Episode 156 (Iteration 6855): Agent passed 0.0 pipes!, Time: 0.057 epsilon: 0.0997 loss: 0.0080\n",
            "Episode 157 (Iteration 6873): Agent passed 0.0 pipes!, Time: 0.061 epsilon: 0.0997 loss: 0.0037\n",
            "Episode 158 (Iteration 6912): Agent passed 0.0 pipes!, Time: 0.131 epsilon: 0.0997 loss: 0.0419\n",
            "Episode 159 (Iteration 6953): Agent passed 1.0 pipes!, Time: 0.140 epsilon: 0.0997 loss: 0.0418\n",
            "Episode 160 (Iteration 6990): Agent passed 0.0 pipes!, Time: 0.140 epsilon: 0.0997 loss: 0.0049\n",
            "Episode 161 (Iteration 7027): Agent passed 0.0 pipes!, Time: 0.147 epsilon: 0.0997 loss: 0.0038\n",
            "Episode 162 (Iteration 7068): Agent passed 1.0 pipes!, Time: 0.143 epsilon: 0.0997 loss: 0.0042\n",
            "Episode 163 (Iteration 7105): Agent passed 0.0 pipes!, Time: 0.126 epsilon: 0.0996 loss: 0.0337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 기존 코드"
      ],
      "metadata": {
        "id": "4_PeL9LW9i_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "from glob import glob\n",
        "from collections import deque\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "\n",
        "from game import Game\n",
        "from utils import init_weights\n",
        "from munch import Munch\n",
        "\n",
        "def train(args):\n",
        "    model = DQN()\n",
        "    if args.use_pretrained:\n",
        "        model = torch.load(\n",
        "            sorted(glob(os.path.join('ckpt', args.tag, '*.pth')))[-1]\n",
        "        )\n",
        "    else:\n",
        "        os.makedirs(os.path.join('ckpt', args.tag), exist_ok = True)\n",
        "        model.apply(init_weights)\n",
        "    model = model.cuda()\n",
        "    start = time.time()\n",
        "\n",
        "    episode = 0\n",
        "    iteration = 0\n",
        "    epsilon = args.epsilon\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "    # instantiate game\n",
        "    game = Game(game=args.game)\n",
        "    high_total_reward = 0\n",
        "\n",
        "    # initialize replay memory\n",
        "    \"\"\"\n",
        "    TO DO\n",
        "\n",
        "    D =\n",
        "    \"\"\"\n",
        "\n",
        "    elapsed_time = 0\n",
        "    action = torch.zeros([model.number_of_actions], dtype=torch.float32)\n",
        "    total_reward = game.reward\n",
        "    terminal = game.game_over()\n",
        "\n",
        "    image_data = game.get_torch_image().cuda()\n",
        "    state = image_data.unsqueeze(0)\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    while iteration < args.iteration:\n",
        "        output = model(state)[0]\n",
        "        action = torch.zeros([model.number_of_actions], dtype=torch.float32)\n",
        "\n",
        "        # epsilon greedy exploration\n",
        "        random_action = False\n",
        "        \"\"\"\n",
        "        TO DO\n",
        "\n",
        "        random_action =\n",
        "        \"\"\"\n",
        "\n",
        "        # Pick action --> random or index of maximum q value\n",
        "        action_index = 0\n",
        "        \"\"\"\n",
        "        TO DO\n",
        "\n",
        "        action_index =\n",
        "        \"\"\"\n",
        "\n",
        "        action[action_index] = 1\n",
        "\n",
        "        elapsed_time = time.time() - start\n",
        "\n",
        "        # get next state and reward\n",
        "        reward = game.act(action_index)\n",
        "        terminal = game.game_over()\n",
        "        image_data_1 = game.get_torch_image().cuda()\n",
        "\n",
        "        state_1 = image_data_1.unsqueeze(0)\n",
        "        action = action.unsqueeze(0).cuda()\n",
        "        reward = torch.from_numpy(np.array([reward], dtype=np.float32)).unsqueeze(0).cuda()\n",
        "\n",
        "        # save transition to replay memory\n",
        "        \"\"\"\n",
        "        TO DO\n",
        "        \"\"\"\n",
        "\n",
        "        # if replay memory is full, remove the oldest transition\n",
        "        \"\"\"\n",
        "        TO DO\n",
        "        \"\"\"\n",
        "\n",
        "        # sample random minibatch\n",
        "        \"\"\"\n",
        "        TO DO\n",
        "        \"\"\"\n",
        "\n",
        "        # get output for the next state\n",
        "        output_1 = model(state_1)\n",
        "\n",
        "        y = reward if terminal else reward + args.gamma * torch.max(output_1)\n",
        "\n",
        "        # calculate with target network\n",
        "        q_value = torch.sum(model(state) * action, dim=1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        y = y.detach()\n",
        "        loss = criterion(q_value, y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        state = state_1\n",
        "        iteration += 1\n",
        "        total_reward += game.reward\n",
        "\n",
        "        args.writer.add_scalar('Train/loss', loss, iteration)\n",
        "\n",
        "        if terminal:\n",
        "            args.writer.add_scalar('Episode/elapsed_time', elapsed_time, episode)\n",
        "            args.writer.add_scalar('Episode/episode', episode, episode)\n",
        "            args.writer.add_scalar('Episode/total_reward', total_reward, episode)\n",
        "            total_reward = 0\n",
        "            game.reset_game()\n",
        "            episode += 1\n",
        "            start = time.time()\n",
        "            print('Episode {} (Iteration {}): Agent passed {} pipes!, Time: {:.3f}'.format(episode, iteration, total_reward, elapsed_time))\n",
        "            if total_reward > high_total_reward:\n",
        "                print('Weight Saved!')\n",
        "                high_total_reward = total_reward\n",
        "                torch.save(model,\n",
        "                           os.path.join('ckpt', args.tag, 'E{:07d}_S{:03d}.pth'.format(episode, int(total_reward)))\n",
        "                           )\n",
        "    print(\"Saving final model\")\n",
        "    torch.save(model,\n",
        "               os.path.join('ckpt', args.tag, 'E_{:07d}_S{:03d}.pth'.format(episode, int(high_total_reward)))\n",
        "               )"
      ],
      "metadata": {
        "id": "5-8NHrQgX1qU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from munch import Munch\n",
        "#parser = argparse.ArgumentParser(description='Deep Q Learning')\n",
        "# Simple parser\n",
        "args = {\n",
        "    \"game\": \"flappy\",\n",
        "    \"gamma\": 0.99,\n",
        "    \"epsilon\": 0.02,\n",
        "    \"iteration\": 1000000,\n",
        "    \"lr\": 1e-4,\n",
        "    \"use_pretrained\": False,\n",
        "    \"tag\": \"dqn\",\n",
        "    \"writer\": \"writer\"\n",
        "}\n",
        "args = Munch(args)\n",
        "\n",
        "args.writer = SummaryWriter(os.path.join('ckpt', args.tag))\n",
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
        "print('GPU Enabled: {}'.format(torch.cuda.is_available()))\n",
        "\n",
        "train(args)\n"
      ],
      "metadata": {
        "id": "nZxWPpsCX7Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=ckpt/dqn"
      ],
      "metadata": {
        "id": "KVY8Ylk86fV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "QabXdcmgqmwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import argparse\n",
        "from glob import glob\n",
        "\n",
        "import torch\n",
        "\n",
        "from game import Game\n",
        "from utils import Recorder\n",
        "\n",
        "def test(args):\n",
        "    model = torch.load(\n",
        "        sorted(glob(os.path.join('ckpt', args.tag, '*.pth')))[-1],\n",
        "        map_location='cpu'\n",
        "    ).eval()\n",
        "    print('Loaded model: {}'.format(sorted(glob(os.path.join('ckpt', args.tag, '*.pth')))[-1]))\n",
        "    # initialize video writer\n",
        "    video_filename = 'output_{}.avi'.format(args.tag)\n",
        "\n",
        "    dict_screen_shape = {\n",
        "        \"flappy\":(288, 512),\n",
        "    }\n",
        "    out = Recorder(video_filename=video_filename, fps=30,\n",
        "                   width=dict_screen_shape[args.game][0],\n",
        "                   height=dict_screen_shape[args.game][1])\n",
        "    total_reward_list = []\n",
        "    time_list = []\n",
        "\n",
        "    rewards = {\n",
        "        \"positive\": 1, # when the plasyer pass the pipe\n",
        "        \"tick\": 0, # at every tick\n",
        "        \"loss\": 0, # when died\n",
        "    }\n",
        "    game = Game(seed=args.seed, game=args.game, rewards=rewards)\n",
        "    for trials in range(10):\n",
        "\n",
        "        elapsed_Time = 0\n",
        "        action = torch.zeros([model.number_of_actions], dtype=torch.float32)\n",
        "        terminal = game.game_over()\n",
        "        start = time.time()\n",
        "        total_reward = 0\n",
        "\n",
        "        image_data = game.get_torch_image()\n",
        "        state = image_data.unsqueeze(0)\n",
        "        while not terminal:\n",
        "            output = model(state)[0]\n",
        "            action = torch.zeros([model.number_of_actions], dtype=torch.float32)\n",
        "            action_index = torch.argmax(output)\n",
        "            total_reward += game.act(action_index)\n",
        "            terminal = game.game_over()\n",
        "            image_data_1 = game.get_torch_image()\n",
        "            state = image_data_1.unsqueeze(0)\n",
        "\n",
        "            out.write(game.get_image())\n",
        "\n",
        "        game.reset_game()\n",
        "        total_reward_list.append(total_reward)\n",
        "        time_list.append(time.time()-start)\n",
        "        print('Game Ended!')\n",
        "        print('Total reward: {} !'.format(total_reward))\n",
        "\n",
        "    # Add summary\n",
        "    out.write_score(sum(total_reward_list), sum(time_list))\n",
        "    out.save()\n",
        "    print('Cumulated Total Reward: {}'.format(sum(total_reward_list)))\n",
        "    print('Total Run Time: {:.3f}'.format(sum(time_list)))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description='Deep Q Learning')\n",
        "    parser.add_argument('--seed', default=42, type=int,\n",
        "                        help='Random seed')\n",
        "    parser.add_argument('--game', default='flappy', type=str,\n",
        "                        help='{flappy}')\n",
        "    parser.add_argument('--tag', default=\"dqn\", type=str,\n",
        "                        help='name to save')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    test(args)"
      ],
      "metadata": {
        "id": "FTb8BqhoqlhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eHhExncNqlc4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}